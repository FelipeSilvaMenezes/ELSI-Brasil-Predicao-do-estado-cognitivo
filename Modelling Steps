Modelagem de variáveis clinicamente acessíveis para predição de comprometimento cognitivo e demência: Uma abordagem de aprendizagem de maquina usando dados do estudo ELSI-Brasil.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/FelipeSilvaMenezes/ELSI-Brasil-Predi-o-do-estado-cognitivo/blob/main/Constru%C3%A7%C3%A3o_dos_modelos_ELSI_2%C2%AA_onda.ipynb

## Bibliotecas utilizadas
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Análise de correlação"""

from scipy.stats import spearmanr

"""Imputação múltipla com KNN"""

from sklearn.impute import KNNImputer
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from statistics import mean

"""Balanceamento"""

!pip install -U imbalanced-learn

"""Divisão dos dados (Treino, validação e teste) e validação cruzada"""

from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_validate

"""Construção do modelo"""

from sklearn.ensemble import RandomForestClassifier

"""Avaliação do modelo"""

from sklearn.metrics import make_scorer, confusion_matrix, accuracy_score, log_loss, recall_score, precision_score, roc_auc_score, cohen_kappa_score

from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    cohen_kappa_score,
    f1_score
)
from imblearn.metrics import geometric_mean_score
import numpy as np

"""## Carregando o banco de dados"""

!pip install openpyxl

# Carregar a planilha preenchida (PCG_preenchida.xlsx)
df = pd.read_excel("caracteristicas-dos-participantes.xlsx")

df.head()

"""# Análise descritiva"""

# Gráfico pizza para a variável "Target"
df = df[df['estad_cog'] != 9]
df["estad_cog"].value_counts().plot(kind="pie", autopct="%1.1f%%", startangle=90)
plt.title("Proporção de Demência e comprometimento cognitivo")
plt.ylabel("")  # Remove o rótulo do eixo Y
plt.show()

"""## Sexo"""

# Quantidade e proporção de homens e mulheres
frequencia_sexo = df['sexo'].value_counts()  # Frequência absoluta
proporcao_sexo = df['sexo'].value_counts(normalize=True) * 100  # Frequência relativa (%)

# Mapear os valores de sexo (1 = Homens, 0 = Mulheres)
mapa_sexo = {1: "Homens", 0: "Mulheres"}

# Combinar resultados em um DataFrame
resultado = pd.DataFrame({
    "Sexo": frequencia_sexo.index.map(mapa_sexo),
    "Quantidade (n)": frequencia_sexo.values,
    "Proporção (%)": proporcao_sexo.values
})

# Exibir os resultados
print("Distribuição por Sexo:")
print(resultado)

"""## Idade(anos), média (DP) e educação , n(%)"""

# Dicionário para mapear as categorias de Educação
mapa_educacao = {
    1: "Analfabeto",
    2: "Menos que Ensino Fundamental",
    3: "Ensino Fundamental Completo",
    4: "Ensino Médio Incompleto",
    5: "Ensino Médio Completo",
    6: "Ensino Superior ou Mais"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para homens e mulheres
resultado_homens = calcular_frequencias(homens, 'educ', mapa_educacao)
resultado_mulheres = calcular_frequencias(mulheres, 'educ', mapa_educacao)

# Exibir os resultados
print("Distribuição da Educação por Sexo:")
print("\nHomens:")
print(resultado_homens)
print("\nMulheres:")
print(resultado_mulheres)

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular média e desvio padrão
def calcular_media_dp(data, coluna, mapa=None):
    if mapa:  # Se houver um mapeamento, aplique-o
        valores_map = data[coluna].map(mapa)
        media = valores_map.mean()
        dp = valores_map.std()
    else:  # Caso contrário, calcule diretamente
        media = data[coluna].mean()
        dp = data[coluna].std()
    return media, dp

# Análise para homens e mulher
media_idade_homens, dp_idade_homens = calcular_media_dp(homens, 'idade')
media_idade_mulheres, dp_idade_mulheres = calcular_media_dp(mulheres, 'idade')

# Exibir os resultados
print("Análise por sexo:")
print("\nHomens:")
print(f"Idade: Média = {media_idade_homens:.2f}, DP = {dp_idade_homens:.2f}")

print("\nMulheres:")
print(f"Idade: Média = {media_idade_mulheres:.2f}, DP = {dp_idade_mulheres:.2f}")

# Dicionário para mapear os valores da variável e9
mapa_idade = {
    1: "50-54",
    2: "55-59",
    3: "60-64",
    4: "65-69",
    5: "70-74",
    6: "75-79",
    7: "80-84",
    8: "85-89",
    9: "90+"
}

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Análise para homens
resultado_idade_homens = calcular_frequencias(homens, 'idade', mapa_idade)

# Análise para mulheres
resultado_idade_mulheres = calcular_frequencias(mulheres, 'idade', mapa_idade)

# Exibir os resultados
print("Distribuição da Cor da Pele por Sexo:")
print("\nHomens:")
print(resultado_idade_homens)

print("\nMulheres:")
print(resultado_idade_mulheres)

"""## Cor da pele, n (%)"""

# Dicionário para mapear os valores da variável e9
mapa_cor_pele = {
    1: "Branca",
    2: "Preta",
    3: "Parda",
    4: "Amarela",
    5: "Indígena"
}

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Análise para homens
resultado_cor_pele_homens = calcular_frequencias(homens, 'cor-pele', mapa_cor_pele)

# Análise para mulheres
resultado_cor_pele_mulheres = calcular_frequencias(mulheres, 'cor-pele', mapa_cor_pele)

# Exibir os resultados
print("Distribuição da Cor da Pele por Sexo:")
print("\nHomens:")
print(resultado_cor_pele_homens)

print("\nMulheres:")
print(resultado_cor_pele_mulheres)

"""## Sem envolvimento ocupacional, n (%)"""

# Dicionário para mapear os valores da variável i1
mapa_ocupacao = {
    0: "Não",
    1: "Sim"
}

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Análise para homens
resultado_ocupacao_homens = calcular_frequencias(homens, 'ocup', mapa_ocupacao)

# Análise para mulheres
resultado_ocupacao_mulheres = calcular_frequencias(mulheres, 'ocup', mapa_ocupacao)

# Exibir os resultados
print("Distribuição da Ocupação por Sexo:")
print("\nHomens:")
print(resultado_ocupacao_homens)

print("\nMulheres:")
print(resultado_ocupacao_mulheres)

"""## IMC, n (%)"""

# Dicionário para mapear os valores do IMC para categorias
mapa_imc = {
    1: "Gravemente abaixo do peso",
    2: "Abaixo do peso",
    3: "Peso normal",
    4: "Sobrepeso",
    5: "Moderadamente obeso (grau I)",
    6: "Obesidade Grave (grau II)",
    7: "Obesidade mórbida (grau III)"
}

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Análise para homens
resultado_homens = calcular_frequencias(homens, 'IMC', mapa_imc)

# Análise para mulheres
resultado_mulheres = calcular_frequencias(mulheres, 'IMC', mapa_imc)

# Exibir os resultados
print("Distribuição do IMC por Sexo:")
print("\nHomens:")
print(resultado_homens)

print("\nMulheres:")
print(resultado_mulheres)

"""## IPAQ e FPM, n(%)"""

# Dicionários para mapear os valores das variáveis
mapa_ipaq = {
    1: "Baixo",
    2: "Moderado",
    3: "Alto"
}

mapa_fpm = {
    1: "Força Baixa",
    2: "Um Pouco Baixa",
    3: "Moderada",
    4: "Um Pouco Alta",
    5: "Alta"
}

# Separar por sexo (1 = homem, 0 = mulher)
homens = df[df['sexo'] == 1]
mulheres = df[df['sexo'] == 0]

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Análise para IPAQ
resultado_ipaq_homens = calcular_frequencias(homens, 'IPAQ', mapa_ipaq)
resultado_ipaq_mulheres = calcular_frequencias(mulheres, 'IPAQ', mapa_ipaq)

# Análise para FPM
resultado_fpm_homens = calcular_frequencias(homens, 'FPM', mapa_fpm)
resultado_fpm_mulheres = calcular_frequencias(mulheres, 'FPM', mapa_fpm)

# Exibir os resultados
print("Distribuição do IPAQ por Sexo:")
print("\nHomens:")
print(resultado_ipaq_homens)
print("\nMulheres:")
print(resultado_ipaq_mulheres)

print("\nDistribuição do FPM por Sexo:")
print("\nHomens:")
print(resultado_fpm_homens)
print("\nMulheres:")
print(resultado_fpm_mulheres)

"""## Hipertensão, Diabates e Colesterol n(%)"""

# Dicionários para mapear as categorias
mapa_hipertensao = {
    0: "Não",
    1: "Sim"
}

mapa_diabetes = {
    0: "Não",
    1: "Sim",
    2: "Sim, apenas durante a gravidez"
}

mapa_colesterol = {
    0: "Não",
    1: "Sim"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_hipertensao_homens = calcular_frequencias(homens, 'hipert', mapa_hipertensao)
resultado_hipertensao_mulheres = calcular_frequencias(mulheres, 'hipert', mapa_hipertensao)

resultado_diabetes_homens = calcular_frequencias(homens, 'diabetes', mapa_diabetes)
resultado_diabetes_mulheres = calcular_frequencias(mulheres, 'diabetes', mapa_diabetes)

resultado_colesterol_homens = calcular_frequencias(homens, 'colest', mapa_colesterol)
resultado_colesterol_mulheres = calcular_frequencias(mulheres, 'colest', mapa_colesterol)

# Exibir os resultados
print("Distribuição de Hipertensão por Sexo:")
print("\nHomens:")
print(resultado_hipertensao_homens)
print("\nMulheres:")
print(resultado_hipertensao_mulheres)

print("\nDistribuição de Diabetes por Sexo:")
print("\nHomens:")
print(resultado_diabetes_homens)
print("\nMulheres:")
print(resultado_diabetes_mulheres)

print("\nDistribuição de Colesterol por Sexo:")
print("\nHomens:")
print(resultado_colesterol_homens)
print("\nMulheres:")
print(resultado_colesterol_mulheres)

"""## Perda de visão (catarata e retinopatia), n(%)"""

# Dicionários para mapear as categorias
mapa_catarata = {
    0: "Não",
    1: "Sim"
}

mapa_retinopatia = {
    0: "Não",
    1: "Sim",
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_catarata_homens = calcular_frequencias(homens, 'catarata', mapa_catarata)
resultado_catarata_mulheres = calcular_frequencias(mulheres, 'catarata', mapa_catarata)

resultado_retinopatia_homens = calcular_frequencias(homens, 'retinop', mapa_retinopatia)
resultado_retinopatia_mulheres = calcular_frequencias(mulheres, 'retinop', mapa_retinopatia)

# Exibir os resultados
print("Distribuição de Catarata por Sexo:")
print("\nHomens:")
print(resultado_catarata_homens)
print("\nMulheres:")
print(resultado_catarata_mulheres)

print("\nDistribuição de Retinopatia por Sexo:")
print("\nHomens:")
print(resultado_retinopatia_homens)
print("\nMulheres:")
print(resultado_retinopatia_mulheres)

"""## Audição e tabagismo, n(%)"""

# Dicionários para mapear as categorias
mapa_audicao = {
    1: "Boa",
    2: "Regular",
    3: "Ruim"
}

mapa_tabagismo = {
    1: "Sim, diariamente",
    2: "Sim, menos que diariamente",
    3: "Não"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_audicao_homens = calcular_frequencias(homens, 'audicao', mapa_audicao)
resultado_audicao_mulheres = calcular_frequencias(mulheres, 'audicao', mapa_audicao)

resultado_tabagismo_homens = calcular_frequencias(homens, 'tabag', mapa_tabagismo)
resultado_tabagismo_mulheres = calcular_frequencias(mulheres, 'tabag', mapa_tabagismo)

# Exibir os resultados
print("Distribuição de audicao por Sexo:")
print("\nHomens:")
print(resultado_audicao_homens)
print("\nMulheres:")
print(resultado_audicao_mulheres)

print("\nDistribuição de tabagismo por Sexo:")
print("\nHomens:")
print(resultado_tabagismo_homens)
print("\nMulheres:")
print(resultado_tabagismo_mulheres)

"""## Consumo excessivo de álcool, n (%)"""

# Dicionários para mapear as categorias
mapa_alcool = {
    1: "Consumo excessivo",
    2: "Sem Consumo excessivo"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_consumo_alcool_homens = calcular_frequencias(homens, 'consu_alc', mapa_alcool)
resultado_consumo_alcool_mulheres = calcular_frequencias(mulheres, 'consu_alc', mapa_alcool)


# Exibir os resultados
print("Distribuição de Consumo excessivo de Álcool por Sexo:")
print("\nHomens:")
print(resultado_consumo_alcool_homens)
print("\nMulheres:")
print(resultado_consumo_alcool_mulheres)

"""## Isolamento social e solidão, n(%)"""

# Dicionários para mapear as categorias
mapa_isolamento_social = {
    0: "contato social",
    1: "isolamento social"
}

mapa_solidao = {
    1: "Nunca",
    2: "Algumas vezes ",
    3: "Sempre"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_isolamento_social_homens = calcular_frequencias(homens, 'isol_soc', mapa_isolamento_social)
resultado_isolamento_social_mulheres = calcular_frequencias(mulheres, 'isol_soc', mapa_isolamento_social)

resultado_solidao_homens = calcular_frequencias(homens, 'solidao', mapa_solidao)
resultado_solidao_mulheres = calcular_frequencias(mulheres, 'solidao', mapa_solidao)

# Exibir os resultados
print("Distribuição de isolamento social por Sexo:")
print("\nHomens:")
print(resultado_isolamento_social_homens)
print("\nMulheres:")
print(resultado_isolamento_social_mulheres)

print("\nDistribuição de solidão por Sexo:")
print("\nHomens:")
print(resultado_solidao_homens)
print("\nMulheres:")
print(resultado_solidao_mulheres)

"""## Satisfação com a vida, média (DP)"""

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular média e desvio padrão para homens
media_homens = homens['satis_vid'].mean()
dp_homens = homens['satis_vid'].std()

# Calcular média e desvio padrão para mulheres
media_mulheres = mulheres['satis_vid'].mean()
dp_mulheres = mulheres['satis_vid'].std()

# Exibir os resultados
print("Satisfação com a vida (Média e DP):")
print(f"\nHomens: Média = {media_homens:.2f}, DP = {dp_homens:.2f}")
print(f"Mulheres: Média = {media_mulheres:.2f}, DP = {dp_mulheres:.2f}")

"""## Sintomas Depressivos (CES-D8), n (%)"""

# Dicionários para mapear as categorias
mapa_CESD = {
    0: "ausência de sintomas",
    1: "sintomas depressivos"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_CESD_homens = calcular_frequencias(homens, 'CES-D8', mapa_CESD)
resultado_CESD_mulheres = calcular_frequencias(mulheres, 'CES-D8', mapa_CESD)


# Exibir os resultados
print("Distribuição de Sintomas depressivos por Sexo:")
print("\nHomens:")
print(resultado_CESD_homens)
print("\nMulheres:")
print(resultado_CESD_mulheres)

"""## Comprometimento cognitivo e Demência"""

# Dicionários para mapear as categorias
mapa_comp_cog_demencia = {
    1: "Cognição Normal",
    2: "Comprometimento cognitivo",
    3: "Demência"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Separar os dados por sexo
homens = df[df['sexo'] == 1]  # Homens
mulheres = df[df['sexo'] == 0]  # Mulheres

# Calcular frequências para cada variável
resultado_cog_global_homens = calcular_frequencias(homens, 'estad_cog', mapa_comp_cog_demencia)
resultado_cog_global_mulheres = calcular_frequencias(mulheres, 'estad_cog', mapa_comp_cog_demencia)


# Exibir os resultados
print("Distribuição de Comprometimento cognitivo e Demência por Sexo:")
print("\nHomens:")
print(resultado_cog_global_homens)
print("\nMulheres:")
print(resultado_cog_global_mulheres)

df['estad_cog'].value_counts()

"""# Análise de correlação

## Dados faltantes
"""

# Verificar se há valores nulos
print(df.isnull().sum())

"""## Correlação de Spearman

"""

# Inicializando matrizes para correlações e p-valores
corr_matrix = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)
pval_matrix = pd.DataFrame(np.zeros((df.shape[1], df.shape[1])), columns=df.columns, index=df.columns)

# Calculando correlações de Spearman e p-valores para cada par de variáveis
for col1 in df.columns:
    for col2 in df.columns:
        mask = df[[col1, col2]].notnull().all(axis=1)  # Máscara para valores não nulos
        if mask.sum() > 1:  # Apenas calcula se houver pelo menos 2 valores válidos
            corr, pval = spearmanr(df.loc[mask, col1], df.loc[mask, col2])
            corr_matrix.loc[col1, col2] = corr
            pval_matrix.loc[col1, col2] = pval
        else:
            corr_matrix.loc[col1, col2] = np.nan
            pval_matrix.loc[col1, col2] = np.nan

# Criando uma máscara para ocultar metade da matriz (triangular superior)
mask_corr = np.triu(np.ones_like(corr_matrix, dtype=bool))
mask_pval = np.triu(np.ones_like(pval_matrix, dtype=bool))

# Configurando o tamanho do gráfico
fig, axes = plt.subplots(2, 1, figsize=(14, 20))  # Dois gráficos, um embaixo do outro

# Mapa de calor para a matriz de correlação
sns.heatmap(
    corr_matrix.astype(float),
    mask=mask_corr,  # Aplicando a máscara
    annot=True,  # Coeficientes de correlação
    fmt=".2f",  # Formato dos números
    cmap="coolwarm",  # Paleta de cores
    linewidths=0.5,  # Linhas divisórias entre as células
    cbar_kws={"label": "Coeficiente de Correlação"},  # Rótulo da barra de cores
    ax=axes[0]  # Primeiro gráfico
)
axes[0].set_title("Matriz de Correlação (Spearman)")

# Mapa de calor para a matriz de p-valores
sns.heatmap(
    pval_matrix.astype(float),
    mask=mask_pval,  # Aplicando a máscara
    annot=True,  # Mostra os p-valores
    fmt=".3f",  # Formato dos números (notação científica para p-valores)
    cmap="YlGnBu",  # Paleta de cores
    linewidths=0.5,  # Linhas divisórias entre as células
    cbar_kws={"label": "p-valor"},  # Rótulo da barra de cores
    ax=axes[1]  # Segundo gráfico
)
axes[1].set_title("Matriz de p-valores (Spearman)")

# Ajustando os espaçamentos
plt.tight_layout()
plt.show()

"""Matriz de correlação e P-valor"""

# Criando a matriz de anotações com correlação e p-valor formatados
annot_matrix = corr_matrix.applymap(lambda x: f"{x:.2f}") + "\n(" + pval_matrix.applymap(lambda x: f"{x:.3f}") + ")"

# Máscara para a parte superior da matriz (opcional)
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Plotando o mapa de calor
plt.figure(figsize=(18, 14))
sns.heatmap(
    corr_matrix,
    mask=mask,  # Máscara para ocultar a parte superior
    annot=annot_matrix,  # Matriz de anotações
    fmt="",  # O formato é tratado na matriz de anotações
    cmap="coolwarm",  # Paleta de cores
    linewidths=1.0,  # Linhas divisórias
    cbar_kws={"label": "Coeficiente de Correlação"},  # Rótulo da barra de cores
    annot_kws={"fontsize": 10, "color": "black"}  # Personalização da fonte
)
plt.title("")
plt.tight_layout()
plt.show()

"""Substituindo os nomes das variáveis por números"""

# Substituindo os nomes das variáveis por números
var_names = list(corr_matrix.columns)
var_dict = {var: f"{i+1}" for i, var in enumerate(var_names)}  # Associa cada variável a um número
corr_matrix.rename(index=var_dict, columns=var_dict, inplace=True)  # Renomeia linhas e colunas
annot_matrix.rename(index=var_dict, columns=var_dict, inplace=True)  # Renomeia a matriz de anotações

# Máscara para a parte superior da matriz (opcional)
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# Plotando o mapa de calor
plt.figure(figsize=(18, 14))
sns.heatmap(
    corr_matrix,
    mask=mask,  # Máscara para ocultar a parte superior
    annot=annot_matrix,  # Matriz de anotações
    fmt="",  # O formato é tratado na matriz de anotações
    cmap="coolwarm",  # Paleta de cores
    linewidths=1.0,  # Linhas divisórias
    cbar_kws={"label": "Coeficiente de Correlação"},  # Rótulo da barra de cores
    annot_kws={"fontsize": 10, "color": "black"}  # Personalização da fonte
)
plt.title("")
plt.tight_layout()
plt.show()

# Exibindo o dicionário de variáveis numeradas para referência
print("Dicionário de variáveis numeradas:")
for var, num in var_dict.items():
    print(f"{num}: {var}")

"""# Pré-processamento de dados

## Verificando dados Faltantes
"""

# Verificando dados ausentes em todo o DataFrame e exibindo a contagem de dados ausentes por coluna

print(df.info())

print("Contagem de Dados Ausentes por Coluna:")
print(df.isnull().sum())

# Calcula valores ausentes e porcentagem
missing = df.isnull().sum()
missing = missing[missing > 0].sort_values(ascending=False)
missing_percent = (missing / len(df)) * 100

# Criando gráfico
plt.figure(figsize=(12, 8))
bars = plt.bar(range(len(missing)), missing, color='black')  # Cor preta

# Adicionando valores absolutos e relativos acima das barras
for bar, abs_val, rel_val in zip(bars, missing, missing_percent):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
             f'{abs_val}\n({rel_val:.1f}%)',
             ha='center', va='bottom', fontsize=10, fontweight='bold')  # Tamanho das letras ajustado

# Configurações do eixo x
plt.xticks(range(len(missing)), range(1, len(missing) + 1), fontsize=10, rotation=45, ha='right')  # Apenas números
plt.ylabel('Quantidade de dados ausentes', fontsize=10)
plt.xlabel('Variaveis preditoras', fontsize=10)
plt.title('', fontsize=12)
plt.tight_layout()

plt.show()

missing = df.isnull().sum()
missing = missing[missing > 0].sort_values()

missing.plot.barh(color='black')
plt.xlabel('Quantidade de Dados Ausentes')
plt.title('Dados Ausentes por Coluna')
plt.show()

df['estad_cog'].info()

"""## Método do Cotovelo "Elbow Method"

"""

erro=[]
n_experiments=30
for experiment in range(0,n_experiments):
   # 3. Simular dados ausentes, removendo 10% dos valores de cada coluna aleatoriamente para teste
   df_missing = df.copy()
   for col in df_missing.columns:
       df_missing.loc[df_missing.sample(frac=0.1).index, col] = np.nan  # Remove 10% dos valores

   # 4. Dividir os dados para treino e teste, mantendo valores completos para comparação
   X_train, X_test = train_test_split(df.dropna(), test_size=0.2, random_state=42)

   # 5. Lista para armazenar os erros de imputação (MAE)
   mae_list = []

   # 6. Definir o intervalo de valores de K (n_neighbors) que será testado
   k_range = range(1, 21)

   # 7. Loop para testar diferentes valores de K
   for k in k_range:
       # Criar o imputador KNN com o valor de K atual
       knn_imputer = KNNImputer(n_neighbors=k, weights="uniform")

       # Imputar os valores ausentes no conjunto com dados faltantes (df_missing)
       df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_missing), columns=df_missing.columns)

       # Calcular o erro médio absoluto (MAE) comparando os valores imputados com os valores reais (X_test)
       mae = mean_absolute_error(X_test, df_imputed.loc[X_test.index])

       # Armazenar o erro MAE para o valor de K atual
       mae_list.append(mae)

       erro.append(mae_list)

temp =[]
mean_error= []
for q in range(0,20):
   for w in range(0,n_experiments):
      temp.append(erro[w][q])
   media = mean(temp)
   mean_error.append(media)
   temp=[]

print('Método do Cotovelo para escolha do número de vizinhos (K) com KNNImputer')
# 8. Plotando o gráfico do método do cotovelo para encontrar o melhor valor de K
plt.figure(figsize=(10, 6))
#plt.plot(k_range, mae_list, marker='o')
plt.plot(k_range, mean_error, marker='o')
plt.title('')
plt.xlabel('Número de vizinhos (K)')
plt.ylabel('Erro Médio Absoluto (EMA)')
plt.grid(True)
plt.show()

# Número de vizinhos ideal (por exemplo, K = 5)
k_ideal = 5

# Plotagem do gráfico
print('Método do Cotovelo para escolha do número de vizinhos (K) no KNNImputer')
plt.figure(figsize=(10, 6))
plt.plot(k_range, mean_error, marker='o', label='Erro Médio Absoluto (EMA)')
plt.axvline(x=k_ideal, color='red', linestyle='--', label=f'K ideal = {k_ideal}')
plt.title('', fontsize=10)
plt.xlabel('Número de vizinhos (K)', fontsize=10)
plt.ylabel('MAE', fontsize=10)
plt.legend(fontsize=10)
plt.grid(True)
plt.show()

# Verificar dados ausentes em todo o DataFrame e exibir contagem de dados ausentes por coluna

print(df.info())

print("Contagem de Dados Ausentes por Coluna:")
print(df.isnull().sum())

"""## Imputação múltipla com KNN (K-nearest neighbors)"""

# Criando o imputador KNN
knn_imputer = KNNImputer(n_neighbors=5, weights="uniform")

# Aplicando o KNN para imputação de valores ausentes
df_imputado = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)

# Convertendo as colunas de float para int
df_imputado = df_imputado.astype(int)

#print(df_imputado.info())

#print("Contagem de Dados Ausentes por Coluna:")
#print(df_imputado.isnull().sum())

"""## Visualização do desequilibrio entre as classes"""

df_imputado['estad_cog'].info()

# Filtrar o DataFrame para excluir as instâncias da classe que deseja remover
y = df_imputado['estad_cog']

# Contagem das instâncias de cada classe
class_counts = y.value_counts()

# Plotando a distribuição das classes
print('Distribuição das Classes')
plt.figure(figsize=(10, 6))
sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')
plt.title('')
plt.xlabel('Classes')
plt.ylabel('Amostra')
plt.show()

# Exibindo as contagens de cada classe
print("Contagem de instâncias por classe:")
print(class_counts)

# Filtrar o DataFrame para excluir as instâncias da classe que deseja remover
y = df_imputado['estad_cog']

# Contagem das instâncias de cada classe
class_counts = y.value_counts()

# Dicionário de mapeamento: números das classes para nomes
mapeamento_classes = {
    1: 'Cognição Normal',
    2: 'Comprometimento cognitivo',
    3: 'Demência'
}


# Plotando a distribuição das classes
print('Distribuição das Classes')
plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x=[mapeamento_classes[c] for c in class_counts.index],  # Mapeia os nomes para o eixo X
    y=class_counts.values,
    palette='Blues'
)

# Adicionando os números sobre as barras
for i, valor in enumerate(class_counts.values):
    ax.text(i, valor + 50, str(valor), ha='center', fontsize=10)  # Posição e tamanho do texto

# Ajustando o gráfico
plt.title('', fontsize=10)  # Tamanho da fonte do título
plt.xlabel('Classes', fontsize=10)  # Tamanho da fonte do eixo X
plt.ylabel('Amostra (n)', fontsize=10)  # Tamanho da fonte do eixo Y
plt.xticks(rotation=15, fontsize=10)  # Rotaciona e ajusta o tamanho da fonte do eixo X
plt.yticks(fontsize=10)  # Ajusta o tamanho da fonte do eixo Y
plt.show()

# Exibindo as contagens de cada classe com os nomes mapeados
print("Contagem de instâncias por classe:")
for classe, quantidade in class_counts.items():
    print(f"{mapeamento_classes[classe]}: {quantidade}")

# Dicionários para mapear as categorias
mapa_comp_cog_demencia = {
    1: "Cognição Normal",
    2: "Comprometimento cognitivo",
    3: "Demência"
}

# Função para calcular frequência absoluta e relativa
def calcular_frequencias(data, coluna, mapa):
    freq_abs = data[coluna].value_counts().sort_index()  # Frequência absoluta
    freq_rel = data[coluna].value_counts(normalize=True).sort_index() * 100  # Frequência relativa (%)

    # Mapear categorias e combinar frequência absoluta e relativa
    resultado = pd.DataFrame({
        "Categoria": freq_abs.index.map(mapa),
        "Frequência (n)": freq_abs.values,
        "Proporção (%)": freq_rel.values
    })
    return resultado

# Calcular frequências para cada variável
resultado_cog_global = calcular_frequencias(df, 'estad_cog', mapa_comp_cog_demencia)


# Exibir os resultados
print("Distribuição de Comprometimento cognitivo e Demência por Sexo:")
print(resultado_cog_global)

"""## Balanceamento das classes com Synthetic Minority Over-sampling Technique + ENN (Edited Nearest Neighbours) - SMOTEENN (SMOTE + ENN)"""

pip install imbalanced-learn

from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import EditedNearestNeighbours

# Separando as features (X) e a variável alvo (y)
X = df_imputado.drop('estad_cog', axis=1)  # Features (atributos)
y = df_imputado['estad_cog']  # Target (variável alvo)

# Definindo a estratégia de amostragem para SMOTE
# Por exemplo, vamos aumentar as classes minoritárias para 70% da classe majoritária
smote_strategy = {
    2: int(0.5 * y.value_counts()[1]),  # Comprometimento Cognitivo
    3: int(0.6 * y.value_counts()[1])   # Demência
}

# Configurando SMOTE e ENN com estratégias de amostragem específicas
smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)
enn = EditedNearestNeighbours(sampling_strategy='auto')  # Subamostra automaticamente

# Aplicando SMOTEENN
smote_enn = SMOTEENN(smote=smote, enn=enn)
X_res, y_res = smote_enn.fit_resample(X, y)

# Criando um novo DataFrame com os dados balanceados
df_balanceado_SMOTEENN = pd.DataFrame(X_res, columns=X.columns)
df_balanceado_SMOTEENN['estad_cog'] = y_res

# Verificando a nova distribuição das classes
print("Distribuição das classes após SMOTEENN:")
print(df_balanceado_SMOTEENN['estad_cog'].value_counts())

# Mapeando os nomes das classes
classe_labels = {
    1: "Cognição Normal",
    2: "Comprometimento Cognitivo",
    3: "Demência"
}

# Substituindo os valores numéricos pelos nomes das classes no DataFrame
df_balanceado_SMOTEENN['estad_cog_label'] = df_balanceado_SMOTEENN['estad_cog'].map(classe_labels)

# Calculando a contagem das classes
contagem_classes = df_balanceado_SMOTEENN['estad_cog_label'].value_counts()

# Plotando a nova distribuição das classes
plt.figure(figsize=(12, 8))
sns.barplot(
    x=contagem_classes.index,
    y=contagem_classes.values,
    palette="Blues"
)

# Adicionando os valores no gráfico
for i, valor in enumerate(contagem_classes.values):
    plt.text(i, valor + 100, str(valor), ha='center', fontsize=12)

# Configurando o gráfico
plt.title('', fontsize=10)
plt.xlabel('Classes', fontsize=10)
plt.ylabel('Amostra (n)', fontsize=10)
plt.xticks(rotation=15)  # Rotação leve para facilitar leitura das classes
plt.show()

# Verificar dados ausentes em todo o DataFrame e exibir contagem de dados ausentes por coluna

print(df_balanceado_SMOTEENN.info())

print("Contagem de Dados Ausentes por Coluna:")
print(df_balanceado_SMOTEENN.isnull().sum())

"""# Experimentos com Random Forest

## Treinamento do modelo com validação cruzada estratificada, validação e teste

### 1 - Experimento: Educ

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_1 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_1 = None
best_score_dfbalanceado_1 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_1 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_1[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_1[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_1[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_1[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_1:
            best_score_dfbalanceado_1 = combined_score
            best_model_dfbalanceado_1 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_1 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_1 = [np.mean(results_dfbalanceado_1[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_1 = [np.mean(results_dfbalanceado_1[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_1 = [np.mean(results_dfbalanceado_1[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_1 = [np.mean(results_dfbalanceado_1[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_1, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_1, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_1, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_1, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_1}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_1:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_1.predict(X_test)
y_test_proba = best_model_dfbalanceado_1.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_1 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_1 = None
best_score_dfdesbalanceado_1 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_1 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_1[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_1[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_1[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_1[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_1:
            best_score_dfdesbalanceado_1 = combined_score
            best_model_dfdesbalanceado_1 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_1 = n_estimators

# Verificar se um modelo foi selecionado
if best_model_dfdesbalanceado_1 is None:
    raise ValueError("Nenhum modelo foi selecionado como o melhor. Verifique os dados ou a lógica de seleção.")

# Calculando médias
train_accuracies_mean_dfdesbalanceado_1 = [np.mean(results_dfdesbalanceado_1[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_1 = [np.mean(results_dfdesbalanceado_1[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_1 = [np.mean(results_dfdesbalanceado_1[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_1 = [np.mean(results_dfdesbalanceado_1[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_1, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_1, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_1, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_1, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_1}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_1:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_1.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_1.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 2 - Experimento: Educ, FPM

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_2 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_2 = None
best_score_dfbalanceado_2 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_2 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_2[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_2[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_2[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_2[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_2:
            best_score_dfbalanceado_2 = combined_score
            best_model_dfbalanceado_2 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_2 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_2 = [np.mean(results_dfbalanceado_2[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_2 = [np.mean(results_dfbalanceado_2[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_2 = [np.mean(results_dfbalanceado_2[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_2 = [np.mean(results_dfbalanceado_2[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_2, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_2, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_2, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_2, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_2}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_2:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_2.predict(X_test)
y_test_proba = best_model_dfbalanceado_2.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF desbalaceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_2 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_2 = None
best_score_dfdesbalanceado_2 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_2 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_2[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_2[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_2[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_2[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_2:
            best_score_dfdesbalanceado_2 = combined_score
            best_model_dfdesbalanceado_2 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_2 = n_estimators

# Verificar se um modelo foi selecionado
if best_model_dfdesbalanceado_2 is None:
    raise ValueError("Nenhum modelo foi selecionado como o melhor. Verifique os dados ou a lógica de seleção.")

# Calculando médias
train_accuracies_mean_dfdesbalanceado_2 = [np.mean(results_dfdesbalanceado_2[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_2 = [np.mean(results_dfdesbalanceado_2[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_2 = [np.mean(results_dfdesbalanceado_2[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_2 = [np.mean(results_dfdesbalanceado_2[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_2, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_2, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_2, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_2, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_2}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_2:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_2.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_2.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 3 - Experimento: Educ, FPM, IPAQ

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_3 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_3 = None
best_score_dfbalanceado_3 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_3 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_3[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_3[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_3[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_3[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_3:
            best_score_dfbalanceado_3 = combined_score
            best_model_dfbalanceado_3 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_3 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_3 = [np.mean(results_dfbalanceado_3[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_3 = [np.mean(results_dfbalanceado_3[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_3 = [np.mean(results_dfbalanceado_3[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_3 = [np.mean(results_dfbalanceado_3[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_3, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_3, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_3, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_3, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_3}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_3:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_3.predict(X_test)
y_test_proba = best_model_dfbalanceado_3.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_3 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_3 = None
best_score_dfdesbalanceado_3 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_3 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_3[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_3[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_3[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_3[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_3:
            best_score_dfdesbalanceado_3 = combined_score
            best_model_dfdesbalanceado_3 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_3 = n_estimators

# Verificar se um modelo foi selecionado
if best_model_dfdesbalanceado_3 is None:
    raise ValueError("Nenhum modelo foi selecionado como o melhor. Verifique os dados ou a lógica de seleção.")

# Calculando médias
train_accuracies_mean_dfdesbalanceado_3 = [np.mean(results_dfdesbalanceado_3[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_3 = [np.mean(results_dfdesbalanceado_3[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_3 = [np.mean(results_dfdesbalanceado_3[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_3 = [np.mean(results_dfdesbalanceado_3[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_3, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_3, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_3, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_3, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_3}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_3:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_3.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_3.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 4 - Experimento: Educ, FPM, IPAQ, CES-D8

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_4 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_4 = None
best_score_dfbalanceado_4 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_4 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_4[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_4[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_4[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_4[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_4:
            best_score_dfbalanceado_4 = combined_score
            best_model_dfbalanceado_4 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_4 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_4 = [np.mean(results_dfbalanceado_4[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_4 = [np.mean(results_dfbalanceado_4[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_4 = [np.mean(results_dfbalanceado_4[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_4 = [np.mean(results_dfbalanceado_4[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_4, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_4, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_4, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_4, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_4}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_4:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_4.predict(X_test)
y_test_proba = best_model_dfbalanceado_4.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_4 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_4 = None
best_score_dfdesbalanceado_4 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_4 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_4[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_4[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_4[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_4[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_4:
            best_score_dfdesbalanceado_4 = combined_score
            best_model_dfdesbalanceado_4 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_4 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_4 = [np.mean(results_dfdesbalanceado_4[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_4 = [np.mean(results_dfdesbalanceado_4[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_4 = [np.mean(results_dfdesbalanceado_4[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_4 = [np.mean(results_dfdesbalanceado_4[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_4, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_4, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_4, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_4, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_4}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_4:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_4.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_4.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 5 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_5 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_5 = None
best_score_dfbalanceado_5 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_5 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_5[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_5[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_5[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_5[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_5:
            best_score_dfbalanceado_5 = combined_score
            best_model_dfbalanceado_5 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_5 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_5 = [np.mean(results_dfbalanceado_5[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_5 = [np.mean(results_dfbalanceado_5[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_5 = [np.mean(results_dfbalanceado_5[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_5 = [np.mean(results_dfbalanceado_5[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_5, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_5, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_5, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_5, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_5}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_5:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_5.predict(X_test)
y_test_proba = best_model_dfbalanceado_5.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_5 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_5 = None
best_score_dfdesbalanceado_5 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_5 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_5[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_5[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_5[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_5[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_5:
            best_score_dfdesbalanceado_5 = combined_score
            best_model_dfdesbalanceado_5 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_5 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_5 = [np.mean(results_dfdesbalanceado_5[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_5 = [np.mean(results_dfdesbalanceado_5[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_5 = [np.mean(results_dfdesbalanceado_5[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_5 = [np.mean(results_dfdesbalanceado_5[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_5, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_5, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_5, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_5, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_5}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_5:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_5.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_5.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 6 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao*, isol_soc

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_6 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_6 = None
best_score_dfbalanceado_6 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_6 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_6[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_6[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_6[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_6[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_6:
            best_score_dfbalanceado_6 = combined_score
            best_model_dfbalanceado_6 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_6 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_6 = [np.mean(results_dfbalanceado_6[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_6 = [np.mean(results_dfbalanceado_6[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_6 = [np.mean(results_dfbalanceado_6[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_6 = [np.mean(results_dfbalanceado_6[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_6, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_6, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_6, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_6, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_6}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_6:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_6.predict(X_test)
y_test_proba = best_model_dfbalanceado_6.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_6 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_6 = None
best_score_dfdesbalanceado_6 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_6 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_6[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_6[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_6[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_6[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_6:
            best_score_dfdesbalanceado_6 = combined_score
            best_model_dfdesbalanceado_6 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_6 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_6 = [np.mean(results_dfdesbalanceado_6[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_6 = [np.mean(results_dfdesbalanceado_6[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_6 = [np.mean(results_dfdesbalanceado_6[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_6 = [np.mean(results_dfdesbalanceado_6[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_6, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_6, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_6, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_6, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_6}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_6:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_6.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_6.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 7 - Experimento: Educ, FPM, IPAQ, CES-D8, isol_soc

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'isol_soc']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_7 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_7 = None
best_score_dfbalanceado_7 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_7 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_7[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_7[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_7[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_7[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_7:
            best_score_dfbalanceado_7 = combined_score
            best_model_dfbalanceado_7 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_7 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_7 = [np.mean(results_dfbalanceado_7[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_7 = [np.mean(results_dfbalanceado_7[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_7 = [np.mean(results_dfbalanceado_7[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_7 = [np.mean(results_dfbalanceado_7[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_7, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_7, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_7, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_7, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_7}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_7:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_7.predict(X_test)
y_test_proba = best_model_dfbalanceado_7.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'isol_soc']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_7 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_7 = None
best_score_dfdesbalanceado_7 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_7 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_7[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_7[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_7[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_7[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_7:
            best_score_dfdesbalanceado_7 = combined_score
            best_model_dfdesbalanceado_7 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_7 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_7 = [np.mean(results_dfdesbalanceado_7[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_7 = [np.mean(results_dfdesbalanceado_7[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_7 = [np.mean(results_dfdesbalanceado_7[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_7 = [np.mean(results_dfdesbalanceado_7[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_7, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_7, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_7, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_7, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_7}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_7:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_7.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_7.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 8 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_8 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_8 = None
best_score_dfbalanceado_8 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_8 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_8[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_8[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_8[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_8[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_8:
            best_score_dfbalanceado_8 = combined_score
            best_model_dfbalanceado_8 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_8 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_8 = [np.mean(results_dfbalanceado_8[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_8 = [np.mean(results_dfbalanceado_8[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_8 = [np.mean(results_dfbalanceado_8[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_8 = [np.mean(results_dfbalanceado_8[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_8, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_8, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_8, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_8, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_8}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_8:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_8.predict(X_test)
y_test_proba = best_model_dfbalanceado_8.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_8 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_8 = None
best_score_dfdesbalanceado_8 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_8 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_8[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_8[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_8[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_8[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_8:
            best_score_dfdesbalanceado_8 = combined_score
            best_model_dfdesbalanceado_8 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_8 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_8 = [np.mean(results_dfdesbalanceado_8[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_8 = [np.mean(results_dfdesbalanceado_8[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_8 = [np.mean(results_dfdesbalanceado_8[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_8 = [np.mean(results_dfdesbalanceado_8[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_8, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_8, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_8, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_8, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_8}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_8:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_8.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_8.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 9 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_9 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_9 = None
best_score_dfbalanceado_9 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_9 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_9[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_9[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_9[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_9[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_9:
            best_score_dfbalanceado_9 = combined_score
            best_model_dfbalanceado_9 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_9 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_9 = [np.mean(results_dfbalanceado_9[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_9 = [np.mean(results_dfbalanceado_9[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_9 = [np.mean(results_dfbalanceado_9[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_9 = [np.mean(results_dfbalanceado_9[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_9, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_9, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_9, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_9, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_9}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_9:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_9.predict(X_test)
y_test_proba = best_model_dfbalanceado_9.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_9 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_9 = None
best_score_dfdesbalanceado_9 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_9 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_9[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_9[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_9[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_9[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_9:
            best_score_dfdesbalanceado_9 = combined_score
            best_model_dfdesbalanceado_9 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_9 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_9 = [np.mean(results_dfdesbalanceado_9[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_9 = [np.mean(results_dfdesbalanceado_9[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_9 = [np.mean(results_dfdesbalanceado_9[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_9 = [np.mean(results_dfdesbalanceado_9[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_9, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_9, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_9, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_9, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_9}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_9:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_9.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_9.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 10 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_10 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_10 = None
best_score_dfbalanceado_10 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_10 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_10[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_10[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_10[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_10[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_10:
            best_score_dfbalanceado_10 = combined_score
            best_model_dfbalanceado_10 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_10 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_10 = [np.mean(results_dfbalanceado_10[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_10 = [np.mean(results_dfbalanceado_10[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_10 = [np.mean(results_dfbalanceado_10[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_10 = [np.mean(results_dfbalanceado_10[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_10, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_10, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_10, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_10, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_10}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_10:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_10.predict(X_test)
y_test_proba = best_model_dfbalanceado_10.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado

"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_10 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_10 = None
best_score_dfdesbalanceado_10 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_10 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_10[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_10[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_10[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_10[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_10:
            best_score_dfdesbalanceado_10 = combined_score
            best_model_dfdesbalanceado_10 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_10 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_10 = [np.mean(results_dfdesbalanceado_10[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_10 = [np.mean(results_dfdesbalanceado_10[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_10 = [np.mean(results_dfdesbalanceado_10[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_10 = [np.mean(results_dfdesbalanceado_10[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_10, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_10, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_10, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_10, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_10}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_10:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_10.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_10.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 11 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_11 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_11 = None
best_score_dfbalanceado_11 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_11 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_11[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_11[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_11[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_11[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_11:
            best_score_dfbalanceado_11 = combined_score
            best_model_dfbalanceado_11 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_11 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_11 = [np.mean(results_dfbalanceado_11[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_11 = [np.mean(results_dfbalanceado_11[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_11 = [np.mean(results_dfbalanceado_11[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_11 = [np.mean(results_dfbalanceado_11[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_11, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_11, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_11, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_11, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_11}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_11:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_11.predict(X_test)
y_test_proba = best_model_dfbalanceado_11.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_11 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_11 = None
best_score_dfdesbalanceado_11 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_11 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_11[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_11[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_11[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_11[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_11:
            best_score_dfdesbalanceado_11 = combined_score
            best_model_dfdesbalanceado_11 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_11 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_11 = [np.mean(results_dfdesbalanceado_11[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_11 = [np.mean(results_dfdesbalanceado_11[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_11 = [np.mean(results_dfdesbalanceado_11[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_11 = [np.mean(results_dfdesbalanceado_11[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_11, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_11, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_11, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_11, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_11}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_11:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_11.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_11.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 12 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_12 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_12 = None
best_score_dfbalanceado_12 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_12 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_12[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_12[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_12[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_12[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_12:
            best_score_dfbalanceado_12 = combined_score
            best_model_dfbalanceado_12 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_12 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_12 = [np.mean(results_dfbalanceado_12[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_12 = [np.mean(results_dfbalanceado_12[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_12 = [np.mean(results_dfbalanceado_12[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_12 = [np.mean(results_dfbalanceado_12[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_12, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_12, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_12, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_12, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_12}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_12:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_12.predict(X_test)
y_test_proba = best_model_dfbalanceado_12.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_12 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_12 = None
best_score_dfdesbalanceado_12 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_12 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_12[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_12[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_12[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_12[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_12:
            best_score_dfdesbalanceado_12 = combined_score
            best_model_dfdesbalanceado_12 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_12 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_12 = [np.mean(results_dfdesbalanceado_12[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_12 = [np.mean(results_dfdesbalanceado_12[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_12 = [np.mean(results_dfdesbalanceado_12[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_12 = [np.mean(results_dfdesbalanceado_12[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_12, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_12, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_12, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_12, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_12}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_12:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_12.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_12.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 13 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag, audicao

Ao acrescentar Audição no modelo, a acurácia e a erro se mantiveram as mesmas, 97,2% e 10,7% respectivamente.

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'audicao']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_13 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_13 = None
best_score_dfbalanceado_13 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_13 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_13[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_13[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_13[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_13[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_13:
            best_score_dfbalanceado_13 = combined_score
            best_model_dfbalanceado_13 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_13 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_13 = [np.mean(results_dfbalanceado_13[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_13 = [np.mean(results_dfbalanceado_13[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_13 = [np.mean(results_dfbalanceado_13[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_13 = [np.mean(results_dfbalanceado_13[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_13, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_13, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_13, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_13, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_13}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_13:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_13.predict(X_test)
y_test_proba = best_model_dfbalanceado_13.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'audicao']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_13 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_13 = None
best_score_dfdesbalanceado_13 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_13 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_13[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_13[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_13[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_13[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_13:
            best_score_dfdesbalanceado_13 = combined_score
            best_model_dfdesbalanceado_13 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_13 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_13 = [np.mean(results_dfdesbalanceado_13[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_13 = [np.mean(results_dfdesbalanceado_13[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_13 = [np.mean(results_dfdesbalanceado_13[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_13 = [np.mean(results_dfdesbalanceado_13[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_13, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_13, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_13, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_13, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_13}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_13:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_13.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_13.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 14 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag, sexo

Audição continuou não acrescentando relevância no desempenho do modelo,
com ou sem a variável a acurácia e a perda se mantiveram as mesma, 98,0% e 0,9% respectivamente.

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_14 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_14 = None
best_score_dfbalanceado_14 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_14 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_14[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_14[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_14[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_14[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_14:
            best_score_dfbalanceado_14 = combined_score
            best_model_dfbalanceado_14 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_14 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_14 = [np.mean(results_dfbalanceado_14[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_14 = [np.mean(results_dfbalanceado_14[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_14 = [np.mean(results_dfbalanceado_14[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_14 = [np.mean(results_dfbalanceado_14[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_14, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_14, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_14, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_14, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_14}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_14:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_14.predict(X_test)
y_test_proba = best_model_dfbalanceado_14.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_14 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_14 = None
best_score_dfdesbalanceado_14 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_14 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_14[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_14[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_14[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_14[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_14:
            best_score_dfdesbalanceado_14 = combined_score
            best_model_dfdesbalanceado_14 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_14 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_14 = [np.mean(results_dfdesbalanceado_14[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_14 = [np.mean(results_dfdesbalanceado_14[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_14 = [np.mean(results_dfdesbalanceado_14[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_14 = [np.mean(results_dfdesbalanceado_14[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_14, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_14, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_14, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_14, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_14}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_14:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_14.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_14.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 15 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag, sexo, idade

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_15 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_15 = None
best_score_dfbalanceado_15 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_15 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_15[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_15[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_15[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_15[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_15:
            best_score_dfbalanceado_15 = combined_score
            best_model_dfbalanceado_15 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_15 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_15 = [np.mean(results_dfbalanceado_15[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_15 = [np.mean(results_dfbalanceado_15[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_15 = [np.mean(results_dfbalanceado_15[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_15 = [np.mean(results_dfbalanceado_15[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_15, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_15, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_15, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_15, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_15}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_15:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_15.predict(X_test)
y_test_proba = best_model_dfbalanceado_15.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_15 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_15 = None
best_score_dfdesbalanceado_15 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_15 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_15[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_15[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_15[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_15[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_15:
            best_score_dfdesbalanceado_15 = combined_score
            best_model_dfdesbalanceado_15 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_15 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_15 = [np.mean(results_dfdesbalanceado_15[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_15 = [np.mean(results_dfdesbalanceado_15[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_15 = [np.mean(results_dfdesbalanceado_15[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_15 = [np.mean(results_dfdesbalanceado_15[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_15, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_15, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_15, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_15, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_15}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_15:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_15.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_15.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 16 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag, sexo, idade, diabetes

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade', 'diabetes']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_16 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_16 = None
best_score_dfbalanceado_16 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_16 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_16[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_16[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_16[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_16[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_16:
            best_score_dfbalanceado_16 = combined_score
            best_model_dfbalanceado_16 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_16 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_16 = [np.mean(results_dfbalanceado_16[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_16 = [np.mean(results_dfbalanceado_16[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_16 = [np.mean(results_dfbalanceado_16[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_16 = [np.mean(results_dfbalanceado_16[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_16, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_16, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_16, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_16, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_16}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_16:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_16.predict(X_test)
y_test_proba = best_model_dfbalanceado_16.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade', 'diabetes']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_16 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_16 = None
best_score_dfdesbalanceado_16 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_16 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_16[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_16[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_16[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_16[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_16:
            best_score_dfdesbalanceado_16 = combined_score
            best_model_dfdesbalanceado_16 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_16 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_16 = [np.mean(results_dfdesbalanceado_16[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_16 = [np.mean(results_dfdesbalanceado_16[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_16 = [np.mean(results_dfdesbalanceado_16[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_16 = [np.mean(results_dfdesbalanceado_16[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_16, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_16, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_16, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_16, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_16}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_16:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_16.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_16.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""### 17 - Experimento: Educ, FPM, IPAQ, CES-D8, solidao, isol_soc, ocup, IMC, cor-pele, satis_vid, tabag, sexo, idade, diabetes, hipert

Desempenho: treino e validação

Df balanceado
"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_balanceado_SMOTEENN[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade', 'diabetes', 'hipert']]  # Features (atributos)
y = df_balanceado_SMOTEENN[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfbalanceado_17 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfbalanceado_17 = None
best_score_dfbalanceado_17 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfbalanceado_17 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfbalanceado_17[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfbalanceado_17[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfbalanceado_17[n_estimators]['train_losses'].append(train_loss)
        results_dfbalanceado_17[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfbalanceado_17:
            best_score_dfbalanceado_17 = combined_score
            best_model_dfbalanceado_17 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfbalanceado_17 = n_estimators

        # Calculando médias
        train_accuracies_mean_dfbalanceado_17 = [np.mean(results_dfbalanceado_17[n]['train_accuracies']) for n in n_estimators_list]
        val_accuracies_mean_dfbalanceado_17 = [np.mean(results_dfbalanceado_17[n]['val_accuracies']) for n in n_estimators_list]
        train_losses_mean_dfbalanceado_17 = [np.mean(results_dfbalanceado_17[n]['train_losses']) for n in n_estimators_list]
        val_losses_mean_dfbalanceado_17 = [np.mean(results_dfbalanceado_17[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfbalanceado_17, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfbalanceado_17, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfbalanceado_17, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfbalanceado_17, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfbalanceado_17}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfbalanceado_17:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfbalanceado_17.predict(X_test)
y_test_proba = best_model_dfbalanceado_17.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""DF Desbalanceado"""

# Divisão dos dados em treino, validação e teste (70% treino, 15% validação, 15% teste)
X = df_imputado[['educ', 'FPM', 'IPAQ', 'CES-D8', 'solidao', 'isol_soc', 'ocup', 'IMC', 'cor-pele', 'satis_vid', 'tabag', 'sexo', 'idade', 'diabetes', 'hipert']]  # Features (atributos)
y = df_imputado[['estad_cog']]  # Target (variável alvo)

# Dividindo em treino (70%) e teste + validação (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)

# Configurações para o número de árvores de decisão
n_estimators_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]

# Configuração da validação cruzada estratificada
rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=30, random_state=42)

# Listas para armazenar resultados de todos os experimentos por n_estimators
results_dfdesbalanceado_17 = {n: {'train_accuracies': [], 'val_accuracies': [], 'train_losses': [], 'val_losses': []} for n in n_estimators_list}

# Inicializar variáveis para armazenar o melhor modelo e seu desempenho
best_model_dfdesbalanceado_17 = None
best_score_dfdesbalanceado_17 = -np.inf  # Inicializar com um valor muito baixo
best_n_estimators_dfdesbalanceado_17 = None

# Loop para testar cada configuração de n_estimators
for n_estimators in n_estimators_list:
    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    # Executar a validação cruzada
    for train_index, val_index in rskf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

        # Treinar o modelo no fold atual
        rf_classifier.fit(X_train_fold, y_train_fold)

        # Previsões no conjunto de treino
        y_train_pred = rf_classifier.predict(X_train_fold)
        y_train_proba = rf_classifier.predict_proba(X_train_fold)

        # Previsões no conjunto de validação
        y_val_pred = rf_classifier.predict(X_val_fold)
        y_val_proba = rf_classifier.predict_proba(X_val_fold)

        # Calcular acurácia e log loss no conjunto de treino
        train_accuracy = accuracy_score(y_train_fold, y_train_pred)
        train_loss = log_loss(y_train_fold, y_train_proba)

        # Calcular acurácia e log loss no conjunto de validação
        val_accuracy = accuracy_score(y_val_fold, y_val_pred)
        val_loss = log_loss(y_val_fold, y_val_proba)

        # Armazenar os resultados
        results_dfdesbalanceado_17[n_estimators]['train_accuracies'].append(train_accuracy)
        results_dfdesbalanceado_17[n_estimators]['val_accuracies'].append(val_accuracy)
        results_dfdesbalanceado_17[n_estimators]['train_losses'].append(train_loss)
        results_dfdesbalanceado_17[n_estimators]['val_losses'].append(val_loss)

        # Calcular uma métrica combinada para encontrar o "melhor modelo"
        combined_score = val_accuracy - val_loss  # Maior acurácia e menor log loss
        if combined_score > best_score_dfdesbalanceado_17:
            best_score_dfdesbalanceado_17 = combined_score
            best_model_dfdesbalanceado_17 = rf_classifier  # Salvar o modelo com melhor desempenho
            best_n_estimators_dfdesbalanceado_17 = n_estimators

# Calculando médias
train_accuracies_mean_dfdesbalanceado_17 = [np.mean(results_dfdesbalanceado_17[n]['train_accuracies']) for n in n_estimators_list]
val_accuracies_mean_dfdesbalanceado_17 = [np.mean(results_dfdesbalanceado_17[n]['val_accuracies']) for n in n_estimators_list]
train_losses_mean_dfdesbalanceado_17 = [np.mean(results_dfdesbalanceado_17[n]['train_losses']) for n in n_estimators_list]
val_losses_mean_dfdesbalanceado_17 = [np.mean(results_dfdesbalanceado_17[n]['val_losses']) for n in n_estimators_list]

# Criar o gráfico estilizado
fig, ax = plt.subplots(1, 2, figsize=(14, 5))

# Gráfico de Acurácia
ax[0].plot(n_estimators_list, train_accuracies_mean_dfdesbalanceado_17, label='Treino', marker='o', color='black')
ax[0].plot(n_estimators_list, val_accuracies_mean_dfdesbalanceado_17, label='Validação', marker='o', linestyle='--', color='orange')
ax[0].set_xlabel('Número de Árvores', fontsize=10)
ax[0].set_ylabel('Acurácia', fontsize=10)
ax[0].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[0].grid(True, linestyle='--', linewidth=0.5)
ax[0].legend(fontsize=10)

# Gráfico de Log Loss
ax[1].plot(n_estimators_list, train_losses_mean_dfdesbalanceado_17, label='Treino', marker='o', color='black')
ax[1].plot(n_estimators_list, val_losses_mean_dfdesbalanceado_17, label='Validação', marker='o', linestyle='--', color='orange')
ax[1].set_xlabel('Número de Árvores', fontsize=10)
ax[1].set_ylabel('Perda Logarítmica', fontsize=10)
ax[1].set_xticks(n_estimators_list)  # Mostrar todos os pontos no eixo X
ax[1].grid(True, linestyle='--', linewidth=0.5)
ax[1].legend(fontsize=10)

# Ajustar layout
plt.tight_layout()
plt.show()



# Exibir informações do melhor modelo
print(f"Melhor número de árvores (n_estimators): {best_n_estimators_dfdesbalanceado_17}")
print(f"Melhor desempenho combinado no conjunto de validação: {best_score_dfdesbalanceado_17:.4f}")


# Avaliar o melhor modelo no conjunto de teste
y_test_pred = best_model_dfdesbalanceado_17.predict(X_test)
y_test_proba = best_model_dfdesbalanceado_17.predict_proba(X_test)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_loss = log_loss(y_test, y_test_proba)

# Exibir os resultados nos conjuntos

print("\nResultados no conjunto de treino:")
print(f"  Acurácia: {train_accuracy:.4f}")
print(f"  Log Loss: {train_loss:.4f}")

print("\nResultados no conjunto de validação:")
print(f"  Acurácia: {val_accuracy:.4f}")
print(f"  Log Loss: {val_loss:.4f}")

print("\nResultados no conjunto de teste:")
print(f"  Acurácia: {test_accuracy:.4f}")
print(f"  Log Loss: {test_loss:.4f}")

"""## Treino, validação e Teste: Modelo 16

### Metricas adiconais: Treino e validação
"""

# Função para calcular métricas
def calculate_metrics(y_true, y_pred, y_proba=None):
    # Acurácia
    accuracy = accuracy_score(y_true, y_pred)

    # Matriz de Confusão
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Sensibilidade (Recall por classe)
    sensitivity_per_class = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)
    sensitivity_general = np.mean(sensitivity_per_class)

    # Especificidade (por classe e geral)
    specificity_per_class = []
    for i in range(len(conf_matrix)):
        tn = np.sum(conf_matrix) - (np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - conf_matrix[i, i])
        fp = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]
        specificity_per_class.append(tn / (tn + fp))
    specificity_general = np.mean(specificity_per_class)

    # Precisão (por classe e geral)
    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
    precision_general = np.mean(precision_per_class)

    # Área sob a curva ROC (AUC por classe e geral)
    if y_proba is not None:
        roc_auc_per_class = roc_auc_score(y_true, y_proba, multi_class='ovr', average=None)
        roc_auc_general = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
    else:
        roc_auc_per_class = None
        roc_auc_general = None

    # Estatística Kappa
    kappa = cohen_kappa_score(y_true, y_pred)

    # Retornar todas as métricas
    return {
        "accuracy": accuracy,
        "confusion_matrix": conf_matrix,
        "sensitivity_per_class": sensitivity_per_class,
        "sensitivity_general": sensitivity_general,
        "specificity_per_class": specificity_per_class,
        "specificity_general": specificity_general,
        "precision_per_class": precision_per_class,
        "precision_general": precision_general,
        "roc_auc_per_class": roc_auc_per_class,
        "roc_auc_general": roc_auc_general,
        "kappa": kappa
    }

# Calcular métricas para o conjunto de treino
train_metrics = calculate_metrics(y_train_fold, y_train_pred, y_proba=best_model_dfbalanceado_16.predict_proba(X_train_fold))

# Calcular métricas para o conjunto de validação
test_metrics = calculate_metrics(y_val_fold, y_val_pred, y_proba=y_val_proba)

# Exibir resultados
def display_metrics(metrics, dataset_name):
    print(f"\nMétricas para o conjunto {dataset_name}:")
    print(f"Acurácia: {metrics['accuracy']:.4f}")
    print(f"Sensibilidade (Recall por classe): {metrics['sensitivity_per_class']}")
    print(f"Sensibilidade (Geral): {metrics['sensitivity_general']:.4f}")
    print(f"Especificidade por classe: {metrics['specificity_per_class']}")
    print(f"Especificidade (Geral): {metrics['specificity_general']:.4f}")
    print(f"Precisão por classe: {metrics['precision_per_class']}")
    print(f"Precisão (Geral): {metrics['precision_general']:.4f}")
    if metrics['roc_auc_per_class'] is not None:
        print(f"Área sob a curva ROC (AUC por classe): {metrics['roc_auc_per_class']}")
        print(f"Área sob a curva ROC (AUC Geral): {metrics['roc_auc_general']:.4f}")
    else:
        print("Área sob a curva ROC (AUC): Não disponível")
    print(f"Estatística Kappa: {metrics['kappa']:.4f}")
    print("\nMatriz de Confusão:")
    print(metrics['confusion_matrix'])

# Exibir métricas para conjunto de treino e teste
display_metrics(train_metrics, "de treino")
display_metrics(test_metrics, "de validação")

# Matriz de confusão
conf_matrix_train = confusion_matrix(y_train_fold, y_train_pred)
print(conf_matrix_train)

# Nomes das classes
class_names = ['Cognição Normal', 'Comprometimento Cognitivo', 'Demência']

# Plot da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Categorias Previstas')
plt.ylabel('Categorias reais')
plt.show()

# Matriz de confusão
conf_matrix_val = confusion_matrix(y_val_fold, y_val_pred)
print(conf_matrix_val)

# Nomes das classes
class_names = ['Cognição Normal', 'Comprometimento Cognitivo', 'Demência']

# Plot da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_val, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Categorias Previstas')
plt.ylabel('Categorias reais')
plt.show()

"""### Metricas adiconais: Teste"""

# Função para calcular métricas
def calculate_metrics(y_true, y_pred, y_proba=None):
    # Acurácia
    accuracy = accuracy_score(y_true, y_pred)

    # Matriz de Confusão
    conf_matrix = confusion_matrix(y_true, y_pred)

    # Sensibilidade (Recall por classe)
    sensitivity_per_class = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)
    sensitivity_general = np.mean(sensitivity_per_class)

    # Especificidade (por classe e geral)
    specificity_per_class = []
    for i in range(len(conf_matrix)):
        tn = np.sum(conf_matrix) - (np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - conf_matrix[i, i])
        fp = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]
        specificity_per_class.append(tn / (tn + fp))
    specificity_general = np.mean(specificity_per_class)

    # Precisão (por classe e geral)
    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
    precision_general = np.mean(precision_per_class)

    # Área sob a curva ROC (AUC por classe e geral)
    if y_proba is not None:
        roc_auc_per_class = roc_auc_score(y_true, y_proba, multi_class='ovr', average=None)
        roc_auc_general = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
    else:
        roc_auc_per_class = None
        roc_auc_general = None

    # Estatística Kappa
    kappa = cohen_kappa_score(y_true, y_pred)

    # Retornar todas as métricas
    return {
        "accuracy": accuracy,
        "confusion_matrix": conf_matrix,
        "sensitivity_per_class": sensitivity_per_class,
        "sensitivity_general": sensitivity_general,
        "specificity_per_class": specificity_per_class,
        "specificity_general": specificity_general,
        "precision_per_class": precision_per_class,
        "precision_general": precision_general,
        "roc_auc_per_class": roc_auc_per_class,
        "roc_auc_general": roc_auc_general,
        "kappa": kappa
    }


# Calcular métricas para o conjunto de validação
test_metrics = calculate_metrics(y_test, y_test_pred, y_proba=y_test_proba)

# Exibir resultados
def display_metrics(metrics, dataset_name):
    print(f"\nMétricas para o conjunto {dataset_name}:")
    print(f"Acurácia: {metrics['accuracy']:.4f}")
    print(f"Sensibilidade (Recall por classe): {metrics['sensitivity_per_class']}")
    print(f"Sensibilidade (Geral): {metrics['sensitivity_general']:.4f}")
    print(f"Especificidade por classe: {metrics['specificity_per_class']}")
    print(f"Especificidade (Geral): {metrics['specificity_general']:.4f}")
    print(f"Precisão por classe: {metrics['precision_per_class']}")
    print(f"Precisão (Geral): {metrics['precision_general']:.4f}")
    if metrics['roc_auc_per_class'] is not None:
        print(f"Área sob a curva ROC (AUC por classe): {metrics['roc_auc_per_class']}")
        print(f"Área sob a curva ROC (AUC Geral): {metrics['roc_auc_general']:.4f}")
    else:
        print("Área sob a curva ROC (AUC): Não disponível")
    print(f"Estatística Kappa: {metrics['kappa']:.4f}")
    print("\nMatriz de Confusão:")
    print(metrics['confusion_matrix'])

# Exibir métricas para conjunto de treino e teste
display_metrics(test_metrics, "de teste")

# Matriz de confusão
conf_matrix_test = confusion_matrix(y_test, y_test_pred)
print(conf_matrix_test)

# Nomes das classes
class_names = ['Cognição Normal', 'Comprometimento Cognitivo', 'Demência']

# Plot da matriz de confusão
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Categorias Previstas')
plt.ylabel('Categorias reais')
plt.show()

"""Importância das features"""

# Importância das features
importances = best_model_dfbalanceado_16.feature_importances_
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({'Preditores': feature_names, 'Importância relativa': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importância relativa', ascending=False)

# Visualizar as 10 features mais importantes
plt.figure(figsize=(8,4))
sns.barplot(x='Importância relativa', y='Preditores', data=feature_importance_df)
plt.title('')
plt.show()

# Dicionário para mapear os nomes das features
feature_mapping = {
    'educ': 'Educação',
    'satis_vid': 'SV',  # Satisfação com a vida abreviado
    'idade': 'Idade',
    'IMC': 'IMC',
    'FPM': 'FPM',
    'cor-pele': 'Cor da pele',
    'IPAQ': 'IPAQ',
    'solidao': 'Solidão',
    'tabag': 'Tabagismo',
    'sexo': 'Sexo',
    'ocup': 'Ocupação',
    'CED-D8': 'CES-D8',
    'diabetes': 'Diabetes',
    'isol_soc': 'IS'
}

# Criando um DataFrame para organizar os dados
feature_names = X_train.columns  # Nomes das variáveis preditoras
mapped_features = [feature_mapping.get(f, f) for f in feature_names]  # Substitui pelos nomes reais
feature_importances = pd.DataFrame({
    'Feature': mapped_features,
    'Importance': importances
})

# Ordenando por importância
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Convertendo as importâncias em porcentagens
feature_importances['Percentage'] = 100 * feature_importances['Importance']

# Plotando o gráfico de barras
plt.figure(figsize=(18, 8))
plt.barh(feature_importances['Feature'], feature_importances['Percentage'], color='black')
plt.xlabel('Importância Relativa', fontsize=10)
plt.ylabel('Preditores', fontsize=10)
plt.gca().invert_yaxis()  # Inverte o eixo para mostrar o mais importante no topo

# Adicionando as porcentagens ao lado de cada barra, com margem ajustada
for index, value in enumerate(feature_importances['Percentage']):
    plt.text(value + 0.1, index, f"{value:.2f}%", va='center')  # Reduzi o deslocamento horizontal

# Remover as linhas verticais do fundo
plt.grid(axis='x', linestyle='', alpha=0)  # Desativa as linhas verticais

# Ajustar margens para garantir que o texto caiba
plt.subplots_adjust(left=0.2, right=0.9)  # Aumentei a margem direita

plt.show()
